<!DOCTYPE html><html lang="en-US" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="How I reduced RASA testing time by 60-80%" /><meta name="author" content="Tai Le" /><meta property="og:locale" content="en_US" /><meta name="description" content="It is quite a long time since I wrote my latest post, and now I come back stronger with a topic that contains both Back-end and AI. It is my journey to customize the Natural Language Understanding (NLU) testing process of the RASA platform to reduce the computation time. In other words, I applied batch prediction in the testing phase, which is currently not available in the pipeline. This post is probably long because I will list things that I modified, so please be patient and follow it until the end." /><meta property="og:description" content="It is quite a long time since I wrote my latest post, and now I come back stronger with a topic that contains both Back-end and AI. It is my journey to customize the Natural Language Understanding (NLU) testing process of the RASA platform to reduce the computation time. In other words, I applied batch prediction in the testing phase, which is currently not available in the pipeline. This post is probably long because I will list things that I modified, so please be patient and follow it until the end." /><link rel="canonical" href="https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/" /><meta property="og:url" content="https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/" /><meta property="og:site_name" content="Tailtq" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-06T00:00:00+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="How I reduced RASA testing time by 60-80%" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Tai Le" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Tai Le"},"dateModified":"2022-11-06T14:45:42+07:00","datePublished":"2022-08-06T00:00:00+07:00","description":"It is quite a long time since I wrote my latest post, and now I come back stronger with a topic that contains both Back-end and AI. It is my journey to customize the Natural Language Understanding (NLU) testing process of the RASA platform to reduce the computation time. In other words, I applied batch prediction in the testing phase, which is currently not available in the pipeline. This post is probably long because I will list things that I modified, so please be patient and follow it until the end.","headline":"How I reduced RASA testing time by 60-80%","mainEntityOfPage":{"@type":"WebPage","@id":"https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/"},"url":"https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/"}</script><title>How I reduced RASA testing time by 60-80% | Tailtq</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Tailtq"><meta name="application-name" content="Tailtq"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-ZB7VQFD7XK"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-ZB7VQFD7XK'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-right"><div id="avatar"> <a href="/" alt="avatar"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Tailtq</a></div><div class="site-subtitle">A software developer who enjoys learning, reading, and building things</div></div><hr><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <span>Home</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <span>About me</span> </a><li class="nav-item"> <a href="/interesting-blogs/" class="nav-link"> <span>Interesting blogs</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <span>Archives</span> </a><li class="nav-item"> <a href="https://github.com/tailtq" class="nav-link" target="_blank"> <span>GitHub</span> </a><li class="nav-item}"> <a href="https://www.linkedin.com/in/tai-le-05124a187/" class="nav-link" target="_blank"> <span>Linkedin</span> </a></ul></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>How I reduced RASA testing time by 60-80%</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>How I reduced RASA testing time by 60-80%</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Tai Le </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Aug 6, 2022, 12:00 AM +0700" prep="on" > Aug 6, 2022 <i class="unloaded">2022-08-06T00:00:00+07:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Nov 6, 2022, 2:45 PM +0700" prefix="Updated " > Nov 6, 2022 <i class="unloaded">2022-11-06T14:45:42+07:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1952 words">10 min</span></div></div><div class="post-content"><p>It is quite a long time since I wrote my latest post, and now I come back stronger with a topic that contains both Back-end and AI. It is my journey to customize the Natural Language Understanding (NLU) testing process of the RASA platform to reduce the computation time. In other words, I applied <strong>batch prediction</strong> in the testing phase, which is currently not available in the pipeline. This post is probably long because I will list things that I modified, so please be patient and follow it until the end.</p><h2 id="1-technology">1. Technology</h2><p>During the process, I only use <strong>Python</strong>, <strong>RASA (v3.1)</strong>, and the <strong>Monkey Patching</strong> technique to customize the RASA package. For people who donâ€™t know about Monkey Patching yet, it is a technique to edit attributes at runtime, it can also be used to override functions/methods in libraries.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2022-08-06/monkey-patching.png" alt="Vargas, 2019" /></p><p>RASA, on the other hand, is a famous platform to build chatbots that currently has 15k stars in its repository. It supports many NLP components to provide the pipeline having the best performance and high accuracy.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/2022-08-06/rasa.png" alt="RASA" /></p><h2 id="2-dietclassifier">2. DietClassifier</h2><p>As I read from the RASA source, each component in the RASA pipeline has two main methods: <code class="language-plaintext highlighter-rouge">train</code> and <code class="language-plaintext highlighter-rouge">process</code>. The <code class="language-plaintext highlighter-rouge">train</code> method will be used in the training process, and the <code class="language-plaintext highlighter-rouge">process</code> one will be used in the testing process as well as when running RASA servers. Therefore, our ultimate goal is to find out why the <code class="language-plaintext highlighter-rouge">process</code> method cannot run in batches and make adjustments to improve it.</p><p>Because the model is often the slowest one, I will check the <code class="language-plaintext highlighter-rouge">DietClassifier</code> class to make improvements for it first. Looking at the code link below, we can see that the <code class="language-plaintext highlighter-rouge">process</code> method receives a list of messages as the parameter and loops through it. So we only need to refactor the <code class="language-plaintext highlighter-rouge">process</code> method, donâ€™t we?</p><p><a href="https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/classifiers/diet_classifier.py#L1018-L1021">https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/classifiers/diet_classifier.py#L1018-L1021</a></p><p>Unfortunately, there is only one message inside the list after adding logging to that function. Therefore, I had to look for where it is called. After reading through many methods, I found out that the data is passed through these methods below first before going through <code class="language-plaintext highlighter-rouge">process</code>:</p><ol><li><code class="language-plaintext highlighter-rouge">rasa.nlu.test.get_eval_data</code>: receive a list of messages, loop through them, and call parse_message.<li><code class="language-plaintext highlighter-rouge">rasa.core.processor.MessageProcessor.parse_message</code>: pass each message to the above method and verify the result.<li><code class="language-plaintext highlighter-rouge">rasa.core.processor.MessageProcessor._parse_message_with_graph</code>: pass each message to the pipeline.</ol><p>The third one is the culprit which sends only one message to the pipeline at a time. Therefore, we need to customize this one first.</p><h4 id="a-rasanlutestget_eval_data">a. rasa.nlu.test.get_eval_data</h4><p>In the code below, I only loop through all of the text, convert its format into messages, and forward them into the <code class="language-plaintext highlighter-rouge">MessageProcessor.parse_message</code> function.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="k">async</span> <span class="k">def</span> <span class="nf">get_eval_data</span><span class="p">(</span><span class="n">processor</span><span class="p">:</span> <span class="n">MessageProcessor</span><span class="p">,</span> <span class="n">test_data</span><span class="p">:</span> <span class="n">TrainingData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">List</span><span class="p">]:</span>
		<span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/test.py#L1246-L1342
    """</span>
    <span class="c1"># measure time
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="p">...</span>
    <span class="n">should_eval_entities</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">.</span><span class="n">entity_examples</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="c1"># TODO: Pass all messages to the processor
</span>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="nc">UserMessage</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">example</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">TEXT</span><span class="p">))</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">.</span><span class="n">nlu_examples</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">processor</span><span class="p">.</span><span class="nf">parse_message</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">only_output_properties</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="c1"># End change
</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
        <span class="c1"># access to the example using the index
</span>        <span class="n">example</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">.</span><span class="n">nlu_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="p">...</span>
</pre></table></code></div></div><h4 id="b-rasacoreprocessormessageprocessorparse_message">b. rasa.core.processor.MessageProcessor.parse_message</h4><p>We still have a lot of work besides editing the <code class="language-plaintext highlighter-rouge">get_eval_data</code> method. You can see that the <code class="language-plaintext highlighter-rouge">parse_message</code> method only receives one message at a time, so we must customize it as well.</p><p>In this function, I only edited the first parameterâ€™s name and looped through the result to verify the format (<code class="language-plaintext highlighter-rouge">_check_for_unseen_features</code>) when it gets returned.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="k">async</span> <span class="k">def</span> <span class="nf">parse_message</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">UserMessage</span><span class="p">],</span> <span class="n">only_output_properties</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/core/processor.py#L619-L646
    """</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">http_interpreter</span><span class="p">:</span>
        <span class="n">parse_data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">http_interpreter</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">parse_data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_parse_message_with_graph</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">only_output_properties</span><span class="p">)</span>

    <span class="c1"># TODO: parse_data before modifying returns only one item.
</span>    <span class="c1">#  Here I return multiple items and loop through them to modify
</span>    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">parse_data</span><span class="p">:</span>
        <span class="n">logger</span><span class="p">.</span><span class="nf">debug</span><span class="p">(</span>
            <span class="s">"Received user message '{}' with intent '{}' "</span>
            <span class="s">"and entities '{}'"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">item</span><span class="p">[</span><span class="s">"text"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s">"intent"</span><span class="p">],</span> <span class="n">item</span><span class="p">[</span><span class="s">"entities"</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_check_for_unseen_features</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parse_data</span>
</pre></table></code></div></div><h4 id="c-rasacoreprocessormessageprocessor_parse_message_with_graph">c. rasa.core.processor.MessageProcessor._parse_message_with_graph</h4><p>In this method, instead of wrapping around a message with a list, I passed all messages to the pipeline (containing multiple components). Then I retrieved the data using <code class="language-plaintext highlighter-rouge">targets</code> via the key <code class="language-plaintext highlighter-rouge">self.model_metadata.nlu_target</code>, looped through them, and edited the results.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_parse_message_with_graph</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">UserMessage</span><span class="p">],</span> <span class="n">only_output_properties</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/core/processor.py#L648-L673
    """</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">graph_runner</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="n">PLACEHOLDER_MESSAGE</span><span class="p">:</span> <span class="n">messages</span><span class="p">},</span>  <span class="c1"># TODO: receive multiple messages and pass to graph_runner
</span>        <span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">model_metadata</span><span class="p">.</span><span class="n">nlu_target</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">parsed_messages</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">model_metadata</span><span class="p">.</span><span class="n">nlu_target</span><span class="p">]</span>
    <span class="c1"># TODO: loop through message and update them
</span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">parsed_message</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">parsed_messages</span><span class="p">):</span>
        <span class="n">parsed_messages</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">TEXT</span><span class="p">:</span> <span class="s">""</span><span class="p">,</span>
            <span class="n">INTENT</span><span class="p">:</span> <span class="p">{</span><span class="n">INTENT_NAME_KEY</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="n">PREDICTED_CONFIDENCE_KEY</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
            <span class="n">ENTITIES</span><span class="p">:</span> <span class="p">[],</span>
            <span class="o">**</span><span class="n">parsed_message</span><span class="p">.</span><span class="nf">as_dict</span><span class="p">(</span><span class="n">only_output_properties</span><span class="o">=</span><span class="n">only_output_properties</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="k">return</span> <span class="n">parsed_messages</span>
</pre></table></code></div></div><h4 id="d-rasanluclassifiersdiet_classifierdietclassifierprocess">d. rasa.nlu.classifiers.diet_classifier.DIETClassifier.process</h4><p>Adjusting these methods above only opens the entry gate for batch processing, there is still a lot of work to do. In <code class="language-plaintext highlighter-rouge">process</code>, we can see that it gets the predictions from <code class="language-plaintext highlighter-rouge">_predict</code> and does post-processing.</p><p><a href="https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/classifiers/diet_classifier.py#L1018-L1039">https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/classifiers/diet_classifier.py#L1018-L1039</a></p><p>There are a few approaches here:</p><ol><li>Modify the <code class="language-plaintext highlighter-rouge">_predict</code> method to accept all messages and the <code class="language-plaintext highlighter-rouge">self.model.run_inference</code> method will separate data into batches and do the batch prediction there.<li>Separate all messages into batches inside the <code class="language-plaintext highlighter-rouge">process</code> method and directly call <code class="language-plaintext highlighter-rouge">self.model.run_inference</code></ol><p>I tried both approaches and found out that only the second one worked. The first approach is very intuitive but it doesnâ€™t work <strong>because the predictions of many batches donâ€™t share the same dimensions, so they cannot stack on top of each other</strong>. I could not figure out the root causes and I did not want to either because I am not the AI Engineer anymore <em>(just a Back-end guy)</em>.</p><p>The second approach is not impossible, we only need to understand the below methods:</p><ol><li><code class="language-plaintext highlighter-rouge">rasa.nlu.classifiers.diet_classifier.DIETClassifier._predict</code>: It only creates the <code class="language-plaintext highlighter-rouge">RasaModelData</code> object so we obviously can move to <code class="language-plaintext highlighter-rouge">process</code>.<li><code class="language-plaintext highlighter-rouge">rasa.utils.tensorflow.models.RasaModel.run_inference</code>: It generates the data generator based on the <code class="language-plaintext highlighter-rouge">RasaModelData</code> object from the previous step, then does prediction concatenation <em>(we have to skip this one because of what I mentioned earlier)</em>.</ol><p>Therefore, here is the new <code class="language-plaintext highlighter-rouge">run_inference</code> method:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">RasaModel_run_inference</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
    <span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/utils/tensorflow/models.py#L280-L322
    """</span>
    <span class="c1"># data_generator is a tuple of 2 elements - input and output.
</span>    <span class="c1"># We only need input, since output is always None and not
</span>    <span class="c1"># consumed by our TF graphs.
</span>    <span class="n">batch_in</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># TODO: Inference a batch of data instead of one message. I also removed the data concatenation between batches
</span>    <span class="n">outputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Text</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_rasa_predict</span><span class="p">(</span><span class="n">batch_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</pre></table></code></div></div><p>And the new <code class="language-plaintext highlighter-rouge">process</code> method is here, you can look at the description in the code. Note that we need to define a global <code class="language-plaintext highlighter-rouge">BATCH_SIZE</code> variable to easily change it.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">DIETClassifier_process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Message</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">TrainingData</span><span class="p">:</span>
    <span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/classifiers/diet_classifier.py#L1018-L1039
    """</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Run DIETClassifier process"</span><span class="p">)</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="c1"># TODO: Chunk messages into multiple smaller chunks with BATCH_SIZE = 256
</span>    <span class="k">global</span> <span class="n">BATCH_SIZE</span>
    <span class="c1"># create RasaModelData just like what _predict does
</span>    <span class="n">model_data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_create_model_data</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_data</span><span class="p">.</span><span class="nf">is_empty</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">messages</span>
    <span class="c1"># separate the data into batches
</span>    <span class="p">(</span><span class="n">data_generator</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">rasa</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">train_utils</span><span class="p">.</span><span class="nf">create_data_generators</span><span class="p">(</span>
        <span class="n">model_data</span><span class="o">=</span><span class="n">model_data</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>
    <span class="n">data_iterator</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">data_generator</span><span class="p">)</span>
    <span class="n">message_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_messages</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># TODO: pass batch of data to model.run_inference2 function and do post-processing here to
</span>            <span class="c1">#  ignore the result concatenation (if not, the concatenation will be handle in RasaModel_run_inference)
</span>            <span class="n">batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">run_inference2</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">total_messages</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s">"diagnostic_data"</span><span class="p">][</span><span class="s">"attention_weights"</span><span class="p">])</span>

            <span class="c1"># TODO: post-processing
</span>            <span class="k">while</span> <span class="n">message_counter</span> <span class="o">&lt;</span> <span class="n">total_messages</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="n">message_counter</span><span class="p">]</span>
                <span class="n">message_out</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">DIAGNOSTIC_DATA</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s">"attention_weights"</span><span class="p">:</span> <span class="n">out</span><span class="p">[</span><span class="n">DIAGNOSTIC_DATA</span><span class="p">][</span><span class="s">"attention_weights"</span><span class="p">][</span><span class="n">message_counter</span> <span class="o">%</span> <span class="n">BATCH_SIZE</span><span class="p">],</span>
                        <span class="s">"text_transformed"</span><span class="p">:</span> <span class="n">out</span><span class="p">[</span><span class="n">DIAGNOSTIC_DATA</span><span class="p">][</span><span class="s">"text_transformed"</span><span class="p">][</span><span class="n">message_counter</span> <span class="o">%</span> <span class="n">BATCH_SIZE</span><span class="p">],</span>
                    <span class="p">},</span>
                <span class="p">}</span> <span class="k">if</span> <span class="n">out</span> <span class="k">else</span> <span class="bp">None</span>

                <span class="k">if</span> <span class="n">out</span> <span class="ow">and</span> <span class="s">"i_scores"</span> <span class="ow">in</span> <span class="n">out</span><span class="p">:</span>
                    <span class="n">message_out</span><span class="p">[</span><span class="s">"i_scores"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s">"i_scores"</span><span class="p">][</span><span class="n">message_counter</span> <span class="o">%</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">out</span> <span class="ow">and</span> <span class="s">"e_entity_ids"</span> <span class="ow">in</span> <span class="n">out</span><span class="p">:</span>
                    <span class="n">message_out</span><span class="p">[</span><span class="s">"e_entity_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s">"e_entity_ids"</span><span class="p">][</span><span class="n">message_counter</span> <span class="o">%</span> <span class="n">BATCH_SIZE</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">...]</span>
                    <span class="n">message_out</span><span class="p">[</span><span class="s">"e_entity_scores"</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s">"e_entity_scores"</span><span class="p">][</span><span class="n">message_counter</span> <span class="o">%</span> <span class="n">BATCH_SIZE</span><span class="p">][</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">...]</span>

                <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">component_config</span><span class="p">[</span><span class="n">INTENT_CLASSIFICATION</span><span class="p">]:</span>
                    <span class="n">label</span><span class="p">,</span> <span class="n">label_ranking</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_label</span><span class="p">(</span><span class="n">message_out</span><span class="p">)</span>
                    <span class="n">message</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">INTENT</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">add_to_output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="n">message</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="s">"intent_ranking"</span><span class="p">,</span> <span class="n">label_ranking</span><span class="p">,</span> <span class="n">add_to_output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">component_config</span><span class="p">[</span><span class="n">ENTITY_RECOGNITION</span><span class="p">]:</span>
                    <span class="n">entities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_entities</span><span class="p">(</span><span class="n">message_out</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>
                    <span class="n">message</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">ENTITIES</span><span class="p">,</span> <span class="n">entities</span><span class="p">,</span> <span class="n">add_to_output</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">out</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">_execution_context</span><span class="p">.</span><span class="n">should_add_diagnostic_data</span><span class="p">:</span>
                    <span class="n">message</span><span class="p">.</span><span class="nf">add_diagnostic_data</span><span class="p">(</span>
                        <span class="n">self</span><span class="p">.</span><span class="n">_execution_context</span><span class="p">.</span><span class="n">node_name</span><span class="p">,</span> <span class="n">message_out</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">DIAGNOSTIC_DATA</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">message_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span> <span class="nb">StopIteration</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"End DIETClassifier: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">messages</span>
</pre></table></code></div></div><h4 id="e-do-monkey-patching">e. Do Monkey Patching</h4><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">rasa.nlu</span> <span class="kn">import</span> <span class="n">test</span>
<span class="kn">from</span> <span class="n">rasa.core.processor</span> <span class="kn">import</span> <span class="n">MessageProcessor</span>
<span class="kn">from</span> <span class="n">rasa.nlu.classifiers.diet_classifier</span> <span class="kn">import</span> <span class="n">DIETClassifier</span>
<span class="kn">from</span> <span class="n">rasa.utils.tensorflow.models</span> <span class="kn">import</span> <span class="n">RasaModel</span>

<span class="n">test</span><span class="p">.</span><span class="n">get_eval_data</span> <span class="o">=</span> <span class="n">get_eval_data</span>
<span class="n">MessageProcessor</span><span class="p">.</span><span class="n">parse_message</span> <span class="o">=</span> <span class="n">parse_message</span>
<span class="n">MessageProcessor</span><span class="p">.</span><span class="n">_parse_message_with_graph</span> <span class="o">=</span> <span class="n">_parse_message_with_graph</span>
<span class="n">DIETClassifier</span><span class="p">.</span><span class="n">process</span> <span class="o">=</span> <span class="n">DIETClassifier_process</span>
<span class="c1"># run_inference will be used when starting the model, so we shouldn't monkey-patch it.
</span><span class="n">RasaModel</span><span class="p">.</span><span class="n">run_inference2</span> <span class="o">=</span> <span class="n">RasaModel_run_inference</span>
</pre></table></code></div></div><h2 id="3-convertfeaturizer">3. ConveRTFeaturizer</h2><p>After customizing <code class="language-plaintext highlighter-rouge">DIETClassifier</code>, I recalled that the time was cut in half, but I had to wait for a long time to see the data passed to the <code class="language-plaintext highlighter-rouge">DIETClassifier</code>. Therefore, I kept investigating and found out that there is another bottleneck in <code class="language-plaintext highlighter-rouge">ConveRTFeaturizer</code>, this component also uses a third-party model. Therefore, we need to adjust this one too.</p><h4 id="a-rasanlufeaturizersdense_featurizerconvert_featurizer-convertfeaturizerprocess">a. rasa.nlu.featurizers.dense_featurizer.convert_featurizer. ConveRTFeaturizer.process</h4><p>In NLU, I only use <code class="language-plaintext highlighter-rouge">TEXT</code> key, so I skipped <code class="language-plaintext highlighter-rouge">ACTION_TEXT</code>. I only used a function to chunk the data and looped through each chunk.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">chunks</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">):</span>
    <span class="s">"""Yield successive n-sized chunks from lst."""</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="o">//</span> <span class="n">n_chunks</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="k">if</span> <span class="n">chunk_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">chunk_size</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">lst</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">lst</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">ConveRTFeaturizer_process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Message</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Message</span><span class="p">]:</span>
    <span class="s">"""
    https://github.com/RasaHQ/rasa/blob/3.1.x/rasa/nlu/featurizers/dense_featurizer/convert_featurizer.py#L379-L395
    """</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">"Run ConveRTFeaturizer process"</span><span class="p">)</span>

    <span class="c1"># TODO: Chunk messages into multiple smaller chunks with BATCH_SIZE = 256
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">global</span> <span class="n">BATCH_SIZE</span>
    <span class="k">for</span> <span class="n">message_chunk</span> <span class="ow">in</span> <span class="nf">chunks</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">attribute</span> <span class="o">=</span> <span class="n">TEXT</span>
        <span class="n">sequence_features</span><span class="p">,</span> <span class="n">sentence_features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_compute_features</span><span class="p">(</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">attribute</span><span class="o">=</span><span class="n">attribute</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_set_features</span><span class="p">(</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">sequence_features</span><span class="p">,</span> <span class="n">sentence_features</span><span class="p">,</span> <span class="n">attribute</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"End ConveRTFeaturizer: </span><span class="si">{</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">messages</span>
</pre></table></code></div></div><h4 id="b-do-monkey-patching">b. Do Monkey Patching</h4><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">rasa.nlu.featurizers.dense_featurizer.convert_featurizer</span> <span class="kn">import</span> <span class="n">ConveRTFeaturizer</span>

<span class="n">ConveRTFeaturizer</span><span class="p">.</span><span class="n">process</span> <span class="o">=</span> <span class="n">ConveRTFeaturizer_process</span>
</pre></table></code></div></div><h2 id="4-run-the-new-command">4. Run the new command</h2><p>I mimic a part of all options that the <code class="language-plaintext highlighter-rouge">rasa test</code> command provides for some specific usage. Assume that the code is placed in the file <code class="language-plaintext highlighter-rouge">run_new_test.py</code>, here is how we can operate it.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre><td class="rouge-code"><pre><span class="c1"># TODO Command:
#  python run_new_test.py \
#       --model models/model.tar.gz \
#       --nlu data \
#       --out results \
#       --batch-size 256 \
#       --no-plot
</span>
<span class="n">parser</span> <span class="o">=</span> <span class="nc">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"-m"</span><span class="p">,</span> <span class="s">"--model"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="nf">get_latest_model</span><span class="p">(</span><span class="n">DEFAULT_MODELS_PATH</span><span class="p">))</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"-u"</span><span class="p">,</span> <span class="s">"--nlu"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"-d"</span><span class="p">,</span> <span class="s">"--domain"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"domain.yml"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--out"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"results"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--batch-size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--no-plot"</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s">"store_true"</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">batch_size</span>
<span class="n">asyncio</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span>
    <span class="nf">test_nlu</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">nlu</span><span class="p">,</span> <span class="n">output_directory</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">out</span><span class="p">,</span> <span class="n">domain_path</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">domain</span><span class="p">,</span> <span class="n">additional_arguments</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"loglevel"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="s">"model"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
        <span class="s">"stories"</span><span class="p">:</span> <span class="s">"."</span><span class="p">,</span>
        <span class="s">"max_stories"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="s">"endpoints"</span><span class="p">:</span> <span class="s">"endpoints.yml"</span><span class="p">,</span>
        <span class="s">"fail_on_prediction_errors"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"url"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="s">"evaluate_model_directory"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"nlu"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">nlu</span><span class="p">,</span>
        <span class="s">"config"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
        <span class="s">"domain"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">domain</span><span class="p">,</span>
        <span class="s">"cross_validation"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"folds"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s">"runs"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s">"percentages"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">],</span>
        <span class="s">"disable_plotting"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">no_plot</span><span class="p">,</span>
        <span class="s">"successes"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"no_errors"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"no_warnings"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
        <span class="s">"out"</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">out</span><span class="p">,</span>
        <span class="s">"errors"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s">"func"</span><span class="p">:</span> <span class="n">run_nlu_test</span>
    <span class="p">})</span>
<span class="p">)</span>
</pre></table></code></div></div><h2 id="5-conclusion">5. Conclusion</h2><p>I applied this code to my project and the time reduced significantly, around 60-80% for all models running on ~10MB datasets. I really enjoyed this task even though it seemed impossible at the beginning, the feeling is extremely like when I step out of my comfort zone.</p><p>Thatâ€™s it. I could modify a small part of the RASAâ€™s pipeline using my curiosity, AI Engineering, and Back-end knowledge. You can do that too by following these steps above and making the magic happens, good luck.</p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/python/" class="post-tag no-text-decoration" >Python</a> <a href="/tags/back-end/" class="post-tag no-text-decoration" >Back-end</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=How I reduced RASA testing time by 60-80% - Tailtq&url=https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=How I reduced RASA testing time by 60-80% - Tailtq&u=https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=How I reduced RASA testing time by 60-80% - Tailtq&url=https://tailtq.github.io/posts/how-i-reduced-rasa-testing-time-by-60-80/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/asynchronous-programming/">Asynchronous Programming</a><li><a href="/posts/streamlit-a-fast-way-to-build-web-applications/">Streamlit - A Fast Way to Build Web Applications</a><li><a href="/posts/modify-facebook-duckling/">Modify Facebook's Duckling</a><li><a href="/posts/tensorflow-with-amd-ubuntu/">Use Tensorflow with AMD GPU on Ubuntu</a><li><a href="/posts/how-i-reduced-rasa-testing-time-by-60-80/">How I reduced RASA testing time by 60-80%</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/back-end/">Back-end</a> <a class="post-tag" href="/tags/database/">Database</a> <a class="post-tag" href="/tags/devops/">DevOps</a> <a class="post-tag" href="/tags/front-end/">Front-end</a> <a class="post-tag" href="/tags/haskell/">Haskell</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/mysql/">MySQL</a> <a class="post-tag" href="/tags/tensorflow/">Tensorflow</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/multithreading-vs-multiprocessing/"><div class="card-body"> <span class="timeago small" > Apr 2, 2022 <i class="unloaded">2022-04-02T00:00:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-threading vs Multi-processing</h3><div class="text-muted small"><p> At some points, we encounter some problems that make our applications tremendously slow. It could be the amount of computation is large, or accessing multiple resources at the time, or too many tas...</p></div></div></a></div><div class="card"> <a href="/posts/caching/"><div class="card-body"> <span class="timeago small" > May 1, 2022 <i class="unloaded">2022-05-01T00:00:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Caching</h3><div class="text-muted small"><p> With an anxious feeling at this moment, I have decided to write a blog article to relieve that feeling. Again, it is all about summarizing things that I have recently worked on, this time is some c...</p></div></div></a></div><div class="card"> <a href="/posts/bridge-pattern/"><div class="card-body"> <span class="timeago small" > Sep 14, 2022 <i class="unloaded">2022-09-14T00:00:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bridge - A pattern that every developer should know</h3><div class="text-muted small"><p> In the environments where I have worked, I personally think my colleaguesâ€™ mindsets and approaches to coding arenâ€™t wise; they just follow their instinct without considering the refactoring factor....</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/web-scraping-with-python/" class="btn btn-outline-primary" prompt="Older"><p>Web Scraping with Python</p></a> <a href="/posts/bridge-pattern/" class="btn btn-outline-primary" prompt="Newer"><p>Bridge - A pattern that every developer should know</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> Â© 2023 <a href="https://twitter.com/username">Tai Le</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/back-end/">Back end</a> <a class="post-tag" href="/tags/database/">Database</a> <a class="post-tag" href="/tags/devops/">DevOps</a> <a class="post-tag" href="/tags/front-end/">Front end</a> <a class="post-tag" href="/tags/haskell/">Haskell</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/mysql/">MySQL</a> <a class="post-tag" href="/tags/tensorflow/">Tensorflow</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://tailtq.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
